{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "def serializeObject(object_,file_name):\n",
    "    file_object = open(file_name,'wb')\n",
    "    pickle.dump(object_, file_object,protocol = 2)\n",
    "    file_object.close()\n",
    "    return\n",
    "def deserializeObject(file_name):\n",
    "    file_object = open(file_name,'rb')\n",
    "    object_ = pickle.load(file_object)\n",
    "    file_object.close() \n",
    "    return object_\n",
    "def read_file(file):\n",
    "    lst = []\n",
    "    with open(file,'r') as f:\n",
    "        for readline in f: \n",
    "            line_strip = readline.strip()\n",
    "            lst.append(line_strip)\n",
    "    return lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_dataset(tweet_file,label_file):\n",
    "    tweets = read_file(tweet_file)\n",
    "    labels = np.array([int(i) for i in read_file(label_file)], dtype = np.int64)\n",
    "    return tweets,labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#root_address = 'offensive/'\n",
    "root_address = './data/hate/'\n",
    "tweets_train, labels_train = read_dataset(root_address+'train_text.txt', root_address+'train_labels.txt')\n",
    "tweets_test, labels_test = read_dataset(root_address+'test_text.txt', root_address+'test_labels.txt')\n",
    "tweets_validation, labels_validation = read_dataset(root_address+'val_text.txt', root_address+'val_labels.txt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model_name = \"cardiffnlp/twitter-roberta-base\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "train_encodings = tokenizer(tweets_train,  padding=True)\n",
    "val_encodings = tokenizer(tweets_validation, padding=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {k: torch.tensor(v[idx]) for k, v in self.encodings.items()}\n",
    "        item[\"labels\"] = torch.tensor([self.labels[idx]])\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "train_dataset = Dataset(train_encodings, labels_train)\n",
    "valid_dataset = Dataset(val_encodings, labels_validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "save_dir = 'results/hate/twitter_roberta_base/augmented_wise/fasttext_0.2/model/model_best'\n",
    "model = AutoModelForSequenceClassification.from_pretrained(save_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "def compute_metrics(pred):\n",
    "  labels = pred.label_ids\n",
    "  preds = pred.predictions.argmax(-1)\n",
    "  # calculate accuracy using sklearn's function\n",
    "  acc = accuracy_score(labels, preds)\n",
    "  return {\n",
    "      'accuracy': acc,\n",
    "  }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "def classification_aacuracy(pred,labels):\n",
    "    cm = confusion_matrix(labels, pred)\n",
    "    accuracy_per_class = cm.diagonal()/cm.sum(axis=1)\n",
    "    accuracy_all, accuracy_0, accuracy_1 = cm.trace()/cm.sum(), accuracy_per_class[0], accuracy_per_class[1]\n",
    "    return accuracy_all, accuracy_0, accuracy_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prediction_single_tweet(text):\n",
    "    inputs = tokenizer(text, padding=True, return_tensors=\"pt\").to(\"cuda\")\n",
    "    outputs = model(**inputs)\n",
    "    probs = outputs[0].softmax(1)\n",
    "    return torch.argmax(probs).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prediction_set(text_set,labels):\n",
    "    global predicted\n",
    "    predicted = [get_prediction_single_tweet(tweet) for tweet in text_set]\n",
    "    return classification_aacuracy(np.array(predicted),labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8476744186046512 0.9225806451612903 0.6541666666666667\n"
     ]
    }
   ],
   "source": [
    "model.cuda()\n",
    "model.eval()\n",
    "test_acc, class_negative, class_positive = get_prediction_set(tweets_test,labels_test)\n",
    "print(test_acc, class_negative, class_positive)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "f9f85f796d01129d0dd105a088854619f454435301f6ffec2fea96ecbd9be4ac"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
